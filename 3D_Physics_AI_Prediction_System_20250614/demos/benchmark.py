#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯• - æµ‹è¯•ç³»ç»Ÿå„ç»„ä»¶çš„æ€§èƒ½
"""

import sys
import os
import time
import numpy as np
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.physics import Cube, PhysicsEngine
from src.ai import AIPredictor
import torch

def benchmark_physics_engine():
    """æµ‹è¯•ç‰©ç†å¼•æ“æ€§èƒ½"""
    print("âš¡ ç‰©ç†å¼•æ“æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    engine = PhysicsEngine()
    cubes = [Cube([i, 10, j], [0, 0, 0]) for i in range(-2, 3) for j in range(-2, 3)]
    
    # é¢„çƒ­
    for _ in range(10):
        engine.step(cubes)
    
    # æ€§èƒ½æµ‹è¯•
    num_steps = 1000
    start_time = time.time()
    
    for _ in range(num_steps):
        engine.step(cubes)
    
    end_time = time.time()
    duration = end_time - start_time
    fps = num_steps / duration
    
    print(f"ç«‹æ–¹ä½“æ•°é‡: {len(cubes)}")
    print(f"æ¨¡æ‹Ÿæ­¥æ•°: {num_steps}")
    print(f"æ€»è€—æ—¶: {duration:.3f}ç§’")
    print(f"ç‰©ç†FPS: {fps:.1f}")
    print(f"æ¯ä¸ªç«‹æ–¹ä½“: {fps/len(cubes):.1f} FPS")

def benchmark_ai_prediction():
    """æµ‹è¯•AIé¢„æµ‹æ€§èƒ½"""
    print("\nğŸ§  AIé¢„æµ‹æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    model_path = 'output/models/quick_physics_predictor.pth'
    if not os.path.exists(model_path):
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
        return
    
    predictor = AIPredictor()
    predictor.load_model(model_path)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    engine = PhysicsEngine()
    cube = Cube([0, 10, 0], [1, 0, 0])
    
    # å»ºç«‹å†å²æ•°æ®
    for _ in range(predictor.sequence_length):
        engine.step([cube])
    
    # é¢„çƒ­
    for _ in range(5):
        predictor.predict_next_states([cube], steps=5)
    
    # æ€§èƒ½æµ‹è¯•
    num_predictions = 100
    prediction_steps = 10
    
    start_time = time.time()
    
    for _ in range(num_predictions):
        prediction = predictor.predict_next_states([cube], steps=prediction_steps)
        engine.step([cube])  # æ›´æ–°å†å²
    
    end_time = time.time()
    duration = end_time - start_time
    fps = num_predictions / duration
    
    print(f"é¢„æµ‹æ¬¡æ•°: {num_predictions}")
    print(f"æ¯æ¬¡é¢„æµ‹æ­¥æ•°: {prediction_steps}")
    print(f"æ€»è€—æ—¶: {duration:.3f}ç§’")
    print(f"é¢„æµ‹FPS: {fps:.1f}")
    print(f"è®¾å¤‡: {predictor.device}")

def benchmark_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•")
    print("-" * 40)
    
    import psutil
    process = psutil.Process(os.getpid())
    
    # åŸºçº¿å†…å­˜
    baseline_memory = process.memory_info().rss / 1024 / 1024
    print(f"åŸºçº¿å†…å­˜: {baseline_memory:.1f} MB")
    
    # åˆ›å»ºç‰©ç†ç³»ç»Ÿ
    engine = PhysicsEngine()
    cubes = [Cube([i, 10, j], [0, 0, 0]) for i in range(-5, 6) for j in range(-5, 6)]
    
    physics_memory = process.memory_info().rss / 1024 / 1024
    print(f"ç‰©ç†ç³»ç»Ÿå†…å­˜: {physics_memory:.1f} MB (+{physics_memory-baseline_memory:.1f} MB)")
    
    # åŠ è½½AIæ¨¡å‹
    model_path = 'output/models/quick_physics_predictor.pth'
    if os.path.exists(model_path):
        predictor = AIPredictor()
        predictor.load_model(model_path)
        
        ai_memory = process.memory_info().rss / 1024 / 1024
        print(f"AIç³»ç»Ÿå†…å­˜: {ai_memory:.1f} MB (+{ai_memory-physics_memory:.1f} MB)")
    
    # è¿è¡Œä¸€æ®µæ—¶é—´æ¨¡æ‹Ÿ
    for _ in range(1000):
        engine.step(cubes)
    
    final_memory = process.memory_info().rss / 1024 / 1024
    print(f"è¿è¡Œåå†…å­˜: {final_memory:.1f} MB")

def benchmark_gpu_vs_cpu():
    """æ¯”è¾ƒGPUå’ŒCPUæ€§èƒ½"""
    print("\nğŸš€ GPU vs CPU æ€§èƒ½å¯¹æ¯”")
    print("-" * 40)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
        return
    
    model_path = 'output/models/quick_physics_predictor.pth'
    if not os.path.exists(model_path):
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
        return
    
    # CPUæµ‹è¯•
    print("æµ‹è¯•CPUæ€§èƒ½...")
    predictor_cpu = AIPredictor(device=torch.device('cpu'))
    predictor_cpu.load_model(model_path)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_sequences = torch.randn(32, 10, 13)  # batch_size=32
    
    start_time = time.time()
    for _ in range(50):
        with torch.no_grad():
            output, _ = predictor_cpu.model(test_sequences)
    cpu_time = time.time() - start_time
    
    # GPUæµ‹è¯•
    print("æµ‹è¯•GPUæ€§èƒ½...")
    predictor_gpu = AIPredictor(device=torch.device('cuda'))
    predictor_gpu.load_model(model_path)
    test_sequences_gpu = test_sequences.cuda()
    
    # GPUé¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            output, _ = predictor_gpu.model(test_sequences_gpu)
    torch.cuda.synchronize()
    
    start_time = time.time()
    for _ in range(50):
        with torch.no_grad():
            output, _ = predictor_gpu.model(test_sequences_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start_time
    
    print(f"CPUæ—¶é—´: {cpu_time:.3f}ç§’")
    print(f"GPUæ—¶é—´: {gpu_time:.3f}ç§’")
    print(f"åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.1f}x")

def run_all_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    print("ğŸ ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    benchmark_physics_engine()
    benchmark_ai_prediction()
    benchmark_memory_usage()
    benchmark_gpu_vs_cpu()
    
    print("\n" + "=" * 50)
    print("âœ… æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    try:
        run_all_benchmarks()
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
